#5. Calculate RMSE and MAE for each model across all 10 folds and plot
collect_metrics(knn_best_fit, metrics = metric_set(rmse, mae))
collect_metrics(lin_fit, metrics = metric_set(rmse, mae))
collect_metrics(tree_fit, metrics = metric_set(rmse, mae))
#plot for KNN
collect_metrics(knn_best_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#plot for lin_fit
collect_metrics(lin_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#plot for tree fit
collect_metrics(tree_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#Find the average RMSE of each model across the 10 resamples
collect_metrics(knn_best_fit, summarize = TRUE) %>%
filter(.metric == "rmse")
collect_metrics(lin_fit, summarize = TRUE)%>%
filter(.metric == "rmse")
collect_metrics(tree_fit, summarize = TRUE)%>%
filter(.metric == "rmse")
#Using lin_fit, which has the lowest RMSE
#first I fit my training data
lin_fit <- lin_wf %>%
fit(data = chicago_train)
preds <-
bind_cols(
chicago_test,
predict(object = lin_fit, new_data = chicago_test)
)
# calculate the rmse on the testing data
rmse(data = preds, truth = ridership, estimate = .pred)
predictions <- predict(lin_fit, new_data = Chicago_implementation)
print(predictions)
View(Chicago_implementation)
View(chicago_test)
library(vip)
lin_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 10)
collect_metrics(knn_best_fit, metrics = metric_set(rmse, mae))
collect_metrics(lin_fit, metrics = metric_set(rmse, mae))
#1. create a recipe using three step functions
chicago_rec <- recipe(ridership ~ ., data = chicago_train) %>%
step_nzv(all_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_holiday(all_date_predictors()) %>%
prep()
obj <- bake(chicago_rec, new_data = NULL)
#2. Create three different model specifications
#first model specification: linear regression
lin_mod <- linear_reg() %>%
set_engine("lm")
#second model specification: decision tree
tree_mod <- decision_tree() %>%
set_engine(engine = "rpart") %>%
set_mode(mode = "regression")
#third model: KNN
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
knn_grid <- grid_regular(neighbors(range = c(1, 17)), levels = 10)
knn_grid
#3. Create 3 different work flows
# For Lm mod
lin_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(lin_mod)
# For tree mod
tree_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(tree_mod)
# For KNN mod
knn_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(knn_mod)
#4. Fit each of the workflows with the folds in step two
#for LM wf
lin_fit <- lin_wf %>%
fit_resamples(resamples = folds)
# for tree wf
tree_fit <- tree_wf %>%
fit_resamples(resamples = folds)
# for KNN mod
knn_fit <- knn_wf %>%
tune_grid(resamples = folds,
grid = knn_grid,
control = control_grid(save_pred = TRUE),
metrics = metric_set(rmse))
knn_fit %>%
collect_metrics()
knn_fit %>%
show_best()
knn_fit %>%
select_best()
#then create a new mod and refit using optimal k
knn_best_mod <-
nearest_neighbor(neighbors = 17) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
knn_best_wf <-
workflow() %>%
add_model(spec = knn_best_mod) %>%
add_recipe(recipe = chicago_rec)
knn_best_fit<-
knn_best_wf %>%
fit_resamples(resamples = folds)
#5. Calculate RMSE and MAE for each model across all 10 folds and plot
collect_metrics(knn_best_fit, metrics = metric_set(rmse, mae))
collect_metrics(metrics = metric_set(rmse, mae))
collect_metrics(knn_best_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#plot for lin_fit
collect_metrics(lin_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#plot for tree fit
collect_metrics(tree_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
library(tidymodels)
library(patchwork)
library(lubridate)
library(tidyverse)
library(themis)
library(recipes)
library(rpart.plot)
library(parsnip)
library(kknn)
# use this url to download the data directly into R
df <- read_csv("https://data.cityofnewyork.us/api/views/43nn-pn8j/rows.csv")
# clean names with janitor
sampled_df <- df %>%
janitor::clean_names()
# create an inspection year variable
sampled_df <- sampled_df %>%
mutate(inspection_date = mdy(inspection_date)) %>%
mutate(inspection_year = year(inspection_date))
# get most-recent inspection
sampled_df <- sampled_df %>%
group_by(camis) %>%
filter(inspection_date == max(inspection_date)) %>%
ungroup()
# subset the data
sampled_df <- sampled_df %>%
select(camis, boro, zipcode, cuisine_description, inspection_date,
action, violation_code, violation_description, grade,
inspection_type, latitude, longitude, council_district,
census_tract, inspection_year, critical_flag) %>%
filter(complete.cases(.)) %>%
filter(inspection_year >= 2017) %>%
filter(grade %in% c("A", "B", "C"))
# create the binary target variable
sampled_df <- sampled_df %>%
mutate(grade = if_else(grade == "A", "A", "Not A")) %>%
mutate(grade = as.factor(grade))
# create extra predictors
sampled_df <- sampled_df %>%
group_by(boro, zipcode, cuisine_description, inspection_date,
action, violation_code, violation_description, grade,
inspection_type, latitude, longitude, council_district,
census_tract, inspection_year)  %>%
mutate(vermin = str_detect(violation_description, pattern = "mice|rats|vermin|roaches")) %>%
summarize(violations = n(),
vermin_types = sum(vermin),
critical_flags = sum(critical_flag == "Y")) %>%
ungroup()
# write the data
write_csv(sampled_df, "restaurant_grades.csv")
#split the data using set seed
set.seed(20201020)
split <- initial_split(
sampled_df,
prop = 0.8,
strata = "grade"
)
#create testing and training sets
sampled_train <- training(split)
sampled_test <- testing(split)
#use step downsample
sampled_rec <- recipe(grade ~ ., data = sampled_train) %>%
step_downsample(grade)
#Create mod using rpart engine
sampled_mod <-
decision_tree() %>%
set_engine(engine = "rpart") %>%
set_mode(mode = "classification")
#create workflow
sampled_wf <- workflow() %>%
add_recipe(sampled_rec) %>%
add_model(sampled_mod)
# fit the model
sampled_fit <- sampled_wf %>%
fit(data = sampled_df)
# create a tree
rpart.plot::rpart.plot(x = sampled_fit$fit$fit$fit)
#create predictions and probability of predictions for the class
predictions <- bind_cols(
sampled_test,
predict(object = sampled_fit, new_data = sampled_test),
predict(object = sampled_fit, new_data = sampled_test, type = "prob")
)
select(predictions, grade, starts_with(".pred"))
#create  a confusion matrix
conf_mat(data = predictions,
truth = grade,
estimate = .pred_class)
#calculate the precision
yardstick::accuracy(data = predictions,
truth = grade,
estimate = .pred_class)
#calculate the sensitivity
yardstick::precision(data = predictions,
truth = grade,
estimate = .pred_class)
library(vip)
sampled_fit %>%
extract_fit_parsnip() %>%
vip(num_features = 10)
#load the data
Chicago_modeling <- Chicago %>%
slice(1:5678)
Chicago_implementation <- Chicago %>%
slice(5679:5698) %>%
select(-ridership)
#create new variables using lubridate
Chicago_modeling$weekday <- wday(Chicago_modeling$date, label = TRUE)
Chicago_modeling$month <- month(Chicago_modeling$date, label = TRUE)
Chicago_modeling$yearday <- yday(Chicago_modeling$date)
Chicago_implementation$weekday <- wday(Chicago_implementation$date, label = TRUE)
Chicago_implementation$month <- month(Chicago_implementation$date, label = TRUE)
Chicago_implementation$yearday <- yday(Chicago_implementation$date)
#set seed and split data
set.seed(20211101)
split <- initial_split(
Chicago_modeling,
strata = "ridership"
)
#create testing and training sets
chicago_train <- training(split)
chicago_test <- testing(split)
chicago_train %>%
ggplot(aes(x = yearday, y = ridership)) +
geom_point(alpha = 0.5) +
labs(title = "Day of Year and Ridership") +
theme_minimal()
#v-fold validation across 10 folds
folds <- vfold_cv(data = chicago_train, v = 10, repeats = 1)
#1. create a recipe using three step functions
chicago_rec <- recipe(ridership ~ ., data = chicago_train) %>%
step_nzv(all_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_holiday(all_date_predictors()) %>%
prep()
obj <- bake(chicago_rec, new_data = NULL)
#2. Create three different model specifications
#first model specification: linear regression
lin_mod <- linear_reg() %>%
set_engine("lm")
#second model specification: decision tree
tree_mod <- decision_tree() %>%
set_engine(engine = "rpart") %>%
set_mode(mode = "regression")
#third model: KNN
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
knn_grid <- grid_regular(neighbors(range = c(1, 17)), levels = 10)
knn_grid
#3. Create 3 different work flows
# For Lm mod
lin_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(lin_mod)
# For tree mod
tree_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(tree_mod)
# For KNN mod
knn_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(knn_mod)
#4. Fit each of the workflows with the folds in step two
#for LM wf
lin_fit <- lin_wf %>%
fit_resamples(resamples = folds)
# for tree wf
tree_fit <- tree_wf %>%
fit_resamples(resamples = folds)
# for KNN mod
knn_fit <- knn_wf %>%
tune_grid(resamples = folds,
grid = knn_grid,
control = control_grid(save_pred = TRUE),
metrics = metric_set(rmse))
knn_fit %>%
collect_metrics()
knn_fit %>%
show_best()
knn_fit %>%
select_best()
#then create a new mod and refit using optimal k
knn_best_mod <-
nearest_neighbor(neighbors = 17) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
knn_best_wf <-
workflow() %>%
add_model(spec = knn_best_mod) %>%
add_recipe(recipe = chicago_rec)
knn_best_fit<-
knn_best_wf %>%
fit_resamples(resamples = folds)
#5. Calculate RMSE and MAE for each model across all 10 folds and plot
collect_metrics(knn_best_fit, metrics = metric_set(rmse, mae))
collect_metrics(metrics = metric_set(rmse, mae))
#1. create a recipe using three step functions
chicago_rec <- recipe(ridership ~ ., data = chicago_train) %>%
step_nzv(all_predictors()) %>%
step_scale(all_numeric_predictors()) %>%
step_normalize(all_numeric_predictors()) %>%
step_holiday(all_date_predictors()) %>%
prep()
obj <- bake(chicago_rec, new_data = NULL)
#2. Create three different model specifications
#first model specification: linear regression
lin_mod <- linear_reg() %>%
set_engine("lm")
#second model specification: decision tree
tree_mod <- decision_tree() %>%
set_engine(engine = "rpart") %>%
set_mode(mode = "regression")
#third model: KNN
knn_mod <- nearest_neighbor(neighbors = tune()) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
knn_grid <- grid_regular(neighbors(range = c(1, 17)), levels = 10)
knn_grid
#3. Create 3 different work flows
# For Lm mod
lin_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(lin_mod)
# For tree mod
tree_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(tree_mod)
# For KNN mod
knn_wf <- workflow() %>%
add_recipe(chicago_rec) %>%
add_model(knn_mod)
#4. Fit each of the workflows with the folds in step two
#for LM wf
lin_fit <- lin_wf %>%
fit_resamples(resamples = folds, metrics = metric_set(rmse, mae))
# for tree wf
tree_fit <- tree_wf %>%
fit_resamples(resamples = folds, metrics = metric_set(rmse, mae))
# for KNN mod
knn_fit <- knn_wf %>%
tune_grid(resamples = folds,
grid = knn_grid,
control = control_grid(save_pred = TRUE),
metrics = metric_set(rmse, mae))
knn_fit %>%
collect_metrics()
knn_fit %>%
show_best()
knn_fit %>%
select_best()
#then create a new mod and refit using optimal k
knn_best_mod <-
nearest_neighbor(neighbors = 17) %>%
set_engine(engine = "kknn") %>%
set_mode(mode = "regression")
knn_best_wf <-
workflow() %>%
add_model(spec = knn_best_mod) %>%
add_recipe(recipe = chicago_rec)
knn_best_fit<-
knn_best_wf %>%
fit_resamples(resamples = folds)
#plot for KNN
collect_metrics(knn_best_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#plot for lin_fit
collect_metrics(lin_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#plot for tree fit
collect_metrics(tree_fit, summarize = FALSE) %>%
filter(.metric == "rmse") %>%
ggplot(aes(id, .estimate, group = .estimator)) +
geom_line() +
geom_point() +
scale_y_continuous()
#Find the average RMSE of each model across the 10 resamples
collect_metrics(knn_best_fit, summarize = TRUE) %>%
filter(.metric == "rmse")
collect_metrics(lin_fit, summarize = TRUE)%>%
filter(.metric == "rmse")
collect_metrics(tree_fit, summarize = TRUE)%>%
filter(.metric == "rmse")
library(dplyr)
library(readr)
library(ggplot2)
rural_shrid_caste <- read_csv("/Users/Juj/Downloads/shrug-secc-mord-rural-csv (1)/secc_rural_shrid.csv")
rural_consumption <- read_csv("/Users/Juj/Downloads/secc_cons_rural_shrid.csv")
rural_merged <-  merge(rural_consumption, rural_shrid_caste, by = 'shrid2', all = FALSE)
sc_share <- as.numeric(rural_merged$sc_share)
variable_class <- class(sc_share)
print(variable_class)
# Create the categorical variable
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks= quantile(rural_merged$sc_share,
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile')) %>%
ggplot(mapping = aes(x = sc_category, y = secc_cons_pc_rural)) +
geom_boxplot() +
labs(x = "Share of Scheduled Caste Population",
y = "Per Capita Monthly Consumption") +
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks= quantile(rural_merged$sc_share,
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile')) %>%
ggplot(mapping = aes(x = sc_category, y = secc_cons_pc_rural)) +
geom_boxplot() +
labs(x = "Share of Scheduled Caste Population",
y = "Per Capita Monthly Consumption")
rural_merged$sc_category <- cut(rural_merged$sc_share,
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks= quantile(rural_merged$sc_share,
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile')) %>%
ggplot(mapping = aes(x = sc_category, y = secc_cons_pc_rural)) +
geom_boxplot() +
labs(x = "Share of Scheduled Caste Population",
y = "Per Capita Monthly Consumption")
rural_merged$sc_category <- cut(rural_merged$sc_share,
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks = quantile(rural_merged$sc_share, probs = c(0.01, 0.25, 0.5, 0.75, 0.99), na.rm = TRUE),
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile'))
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks = quantile(rural_merged$sc_share, probs = c(0.01, 0.25, 0.5, 0.75, 0.99), na.rm = TRUE) + 1e-10, # Adding a small jitter
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile'))
unique_breaks <- unique(quantile(rural_merged$sc_share, probs = c(0.01, 0.25, 0.5, 0.75, 0.99), na.rm = TRUE))
print(unique_breaks)
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks = quantile(rural_merged$sc_share, probs = c(0.01, 0.25, 0.5, 0.75, 0.99), na.rm = TRUE),
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile'))
rural_merged$sc_share <- as.numeric(rural_merged$sc_share)
variable_class <- class(sc_share)
print(variable_class)
rural_merged$sc_share <- as.numeric(rural_merged$sc_share)
variable_class <- class(rural_merged$sc_share)
print(variable_class)
# Create the categorical variable
unique_breaks <- unique(quantile(rural_merged$sc_share, probs = c(0.01, 0.25, 0.5, 0.75, 0.99), na.rm = TRUE))
print(unique_breaks)
rural_merged$sc_category <- cut(rural_merged$sc_share,
breaks = quantile(rural_merged$sc_share, probs = c(0.01, 0.25, 0.5, 0.75, 0.99), na.rm = TRUE),
labels = c('1st Percentile', '25th Percentile', '50th Percentile', '75th Percentile', '99th Percentile'))
knitr::opts_chunk$set(echo = TRUE)
#import secc urban dataset
secc_urban <- read_csv("secc_cons_urban_shrid.csv")
library(readr)
library(dplyr)
library(ggplot2)
rural_shrid_caste <- read_csv("/Users/Juj/Downloads/shrug-secc-mord-rural-csv (1)/secc_rural_shrid.csv")
rural_consumption <- read_csv("/Users/Juj/Downloads/secc_cons_rural_shrid.csv")
rural_merged <-  merge(rural_consumption, rural_shrid_caste, by = 'shrid2', all = FALSE)
rural_merged %>%
ggplot(mapping = aes(x = sc_share, y = secc_cons_pc_rural)) +
geom_quantile() +
labs(x = "Share of Scheduled Caste Population",
y = "Per Capita Monthly Consumption")
library(ggplot2)
rural_shrid_caste <- read_csv("/Users/Juj/Downloads/shrug-secc-mord-rural-csv (1)/secc_rural_shrid.csv")
rural_consumption <- read_csv("/Users/Juj/Downloads/secc_cons_rural_shrid.csv")
rural_merged <-  merge(rural_consumption, rural_shrid_caste, by = 'shrid2', all = FALSE)
rural_merged %>%
ggplot(mapping = aes(x = sc_share, y = secc_cons_pc_rural)) +
geom_quantile() +
labs(x = "Share of Scheduled Caste Population",
y = "Per Capita Monthly Consumption")
